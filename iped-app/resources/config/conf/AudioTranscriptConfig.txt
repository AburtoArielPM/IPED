# Audio transcript task config file.

# Default implementation uses Vosk transcription on local CPU (slow and medium quality).
# We include small portable models for 'en' and 'pt-BR', if you want to use a different language model
# you should download it from https://alphacephei.com/vosk/models and put in 'models/vosk/[lang]' folder.
implementationClass = iped.engine.task.transcript.VoskTranscriptTask

# Uses a local wav2vec2 implementation for transcription. Accuracy is much better than most Vosk models.
# This is AT LEAST 1 order of magnitude slower than Vosk on high end CPUs. Using a good GPU is highly recommended!
# You must install 'huggingsound' library in iped internal python to enable this:
# pip install huggingsound
# In the first run, it will download the model defined in 'huggingFaceModel' param below, you must configure it.
#implementationClass = iped.engine.task.transcript.Wav2Vec2TranscriptTask

# If you want to use the Microsoft Azure service implementation, comment above and uncomment below.
# You must pass your subscription key using command line parameter -XazureSubscriptionKey=XXXXXXXX
# implementationClass = iped.engine.task.transcript.MicrosoftTranscriptTask

# If you want to use the Google Service implementation, comment above and uncomment below.
# You must include google-cloud-speech-1.22.5.jar AND ITS DEPENDENCIES in plugins folder.
# You can download an all-in-one jar from https://gitlab.com/iped-project/iped-maven/-/blob/master/com/google/cloud/google-cloud-speech/1.22.5-shaded/google-cloud-speech-1.22.5-shaded.jar
# Finally you must set environment variable GOOGLE_APPLICATION_CREDENTIALS pointing to your credential file
#implementationClass = iped.engine.task.transcript.GoogleTranscriptTask

# HuggingFace model card to be used by Wav2Vec2TranscriptTask. You must uncomment one of them.
# First and second models below are for pt-BR.
# huggingFaceModel = lgris/bp_400h_xlsr2_300M
# huggingFaceModel = jonatasgrosman/wav2vec2-large-xlsr-53-portuguese
# huggingFaceModel = jonatasgrosman/wav2vec2-large-xlsr-53-english
# huggingFaceModel = jonatasgrosman/wav2vec2-large-xlsr-53-german
# huggingFaceModel = jonatasgrosman/wav2vec2-large-xlsr-53-italian
# huggingFaceModel = jonatasgrosman/wav2vec2-large-xlsr-53-spanish

# Bigger model, better on tested data sets, but uses more RAM and is ~2x slower.
# There are similar models for other languages in the same repo.
# huggingFaceModel = jonatasgrosman/wav2vec2-xls-r-1b-portuguese


# Minimum word score to include the word in transcription result. Applies just to Vosk implementation.
minWordScore = 0.5

# Specific for Microsoft service. Replace with your own subscription region identifier from here: https://aka.ms/speech/sdkregion
serviceRegion = brazilsouth

# Language model(s) to use when processing audios. 'auto' uses the 'locale' set on LocalConfig.txt
# You can specify one or two languages separated by ; if Microsoft or Google are used.
# Vosk implementation accepts just one language for now.
# Wav2Vec2 implementation doesn't use this, you must set 'huggingFaceModel' above
# Setting more than 1 lang model can result in wrong language detection.
language = auto

# Depending on your subscription, Microsoft limits the number of max concurrent requests
maxConcurrentRequests = 100

# Depending on your subscription, Google limits your request rate (per minute or per second).
requestIntervalMillis = 67

# Command to convert audios to wav before transcription. Do not change $INPUT or $OUTPUT params.
convertCommand = mplayer -benchmark -vo null -vc null -srate 16000 -af format=s16le,channels=1 -ao pcm:fast:file=$OUTPUT $INPUT

# Mime types or supertypes to process. If you want to add videos use ; as separator and update 'convertCommand'.
mimesToProcess = audio

# Max number of seconds to wait until each audio second is transcribed. A minimum of 10s is always wait.
timeout = 3